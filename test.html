

<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="icon" href="../../favicon.ico"> -->

    <!-- <title>Jerry Li</title> -->

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="css/starter-template.css" rel="stylesheet">


    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    

    <div class="container">

    	<div class="row">
            <div class="col-lg-12">
                <h1 class="page-header">Vatsal Sharan
                    <!-- <small>Assistant Professor</small> -->
                </h1>
            </div>
        </div>

      <div class="row">
        <div class="col-md-2">
		<div class="photo">
	  		<img id="picture" src="docs/vatsal.jpg" style="width:240px;height:330px;" class="photo_pic"/>
		</div>      
		</div>
        <div class="col-md-5 col-md-offset-1">
<br>
		  <font size="3">
		  <p>Ph.D. candidate,<br>
		  Stanford University<br>
          <a href="docs/Vatsal-Resume.pdf">(Resume)</a>
		  </p>
		  <p>Gates Computer Science Building, Room 480,<br>
		  353 Jane Stanford Way,<br>
		  Stanford, CA 94305
		  </p>
		  <p>
	 	  vsharan at stanford dot edu
		  </p>
		  </font>
		</div>
      </div>
      <div class="voffset1"></div>
      <div class="row">
        <div class="span12">
		<section id="about">

	    <h3>About</h3>
<!--  	  <p>
	    I am a research scientist in the <a href="https://www.microsoft.com/en-us/research/group/mlog/">Machine Learning and Optimization Group</a> at Microsoft Research Redmond.</p>
	  <p>
	    In Fall 2018 I was the VMware Research Fellow at the <a href="https://simons.berkeley.edu/">Simons Institute</a>.
	    I did my Ph.D at  <a href="http://www.mit.edu">MIT</a>, where I was fortunate to work with <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>. I also did my masters at MIT under the wonderful supervision of <a href="http://people.csail.mit.edu/shanir/"> Nir Shavit</a>. </p>
<p>
My primary research interests are in learning theory and distributed algorithms, but I am broadly interested in  many other things in TCS. 
I particularly like applications of analysis and analytic techniques to TCS problems. 
</p>
<p>
As an  undergrad at the <a href="http://www.uw.edu">University of Washington</a>, I worked on complexity of branching programs, and how we could prove hardness of techniques used for  naturally arising learning problems in database theory and AI.</p>
<p>
In my free time I enjoy being remarkably mediocre at ultimate frisbee, chess, and piano, amongst other things.
</p> -->
			<p>
            I am a final year graduate student at Stanford, where I am fortunate to be advised by <a href="http://theory.stanford.edu/~valiant/">Greg Valiant</a>. I am a part of the <a href="http://theory.stanford.edu/main/index.shtml">Theory Group</a> and the <a href="http://statsml.stanford.edu/">Statistical Machine Learning Group</a> at Stanford, and have had the good fortune of collaborating with amazing researchers from both these groups. I am broadly interested in the theory and practice of machine learning, here are some questions which I like to think about:
            <ul>
            <li>What is the role of memory in learning and optimization? How do we learn concepts with small memory? Are there inherent trade-offs between the amount of memory required for learning or optimization, and the amount of data or computation required?
            <li> How can we ensure that trained models are robust even when evaluated on data distributions which differ from the original training distribution?
            <li> What is the relationship between learning an unknown distribution <i>D</i>, testing properties of <i>D</i>, and generating new samples from <i>D</i>?
            </ul>
            
            I spent summer 2017 as an intern at VMware Research with <a href="https://research.vmware.com/researchers/parikshit-gopalan">Parikshit Gopalan</a> and <a href="https://udiwieder.wordpress.com/">Udi Wieder</a>, and summer 2019 as an intern at Google with <a href="http://theory.stanford.edu/~rinap/">Rina Panigrahy</a>. A long time ago, I spent four wonderful years as an undergraduate at <a href="https://www.iitk.ac.in">IIT Kanpur</a> and worked with <a href="https://www.iitk.ac.in/ee/people/fac-pages/rkbansal.shtml">R.K. Bansal</a> on my final year project.
			</p>
		</section>

<!-- <section id = "teaching">
<h3>Teaching</h3>
<p>I am teaching a course on robust machine learning at UW in Fall 2019! See the <a href="robust-ml-fall19.html">course website</a> for more details.</p><br>

I am fortunate to have supervised the following amazing junior researchers:
<ul>
	<li><b><a href = "https://hadisalman.com/home">Hadi Salman</a></b> (MSR AI Resident, 2018&ndash;2019). </li>
	<li><b><a href = "http://people.csail.mit.edu/sitanc/">Sitan Chen</a></b> (Research Intern, Summer 2019).</li>
</ul>


	  </section> -->

	  <section id="papers">
	    <h3>Publications</h3>
	    (asterisk indicates joint or alphabetical authorship)<br><br>
	    <ol>

	    	<li><p><strong><div style="font-size:15px;">Sample Amplification: Increasing Dataset Size even when Learning is Impossible</div></strong>
		 	Brian Axelrod*, Shivam Garg*, Vatsal Sharan*, Gregory Valiant*<br>
		 	<i>arXiv preprint arXiv:1904.12053, 2019</i><br>
		 	<a data-toggle="collapse" href="#collapseExample1"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1904.12053.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1904.12053"><b>arXiv</b></a>
		 	</p></li>

		 	<div class="collapse" id="collapseExample1">
			   <i>Is learning a distribution always necessary for generating new samples from the distribution?</i> We introduce the problem of "sample amplification": given <i>n</i> independent draws from a distribution, <i>D</i> , to what extent is it possible to output a set of <i>m > n </i>  datapoints that are indistinguishable from <i>m</i>  i.i.d. draws from <i>D</i> ? Curiously, we show that nontrivial amplification is often possible in the regime where <i>n</i>  is too small to learn <i>D</i>  to any nontrivial accuracy.<br><br>
			</div>
            
            <li><p><strong><div style="font-size:15px;">PIDForest: Anomaly Detection via Partial Identification</div></strong>
                Parikshit Gopalan*, Vatsal Sharan*, Udi Wieder*<br>
                <i> To appear in NeurIPS, 2019 <b>(Spotlight)</b></i><br>
                <a data-toggle="collapse" href="#collapseExample2"><b>abstract</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample2">
                We propose a definition of an anomaly that captures the intuition that anomalies are easy to distinguish from the overwhelming majority of points by relatively few attribute values: we call this partial identification. Formalizing this intuition, we propose a geometric anomaly measure for a point that we call PIDScore, which measures for the minimum density of data points over all subcubes containing the point. We present PIDForest: a random forest based algorithm that finds anomalies based on this definition, and show that it performs favorably in comparison to several popular anomaly detection methods, across a broad range of benchmarks.<br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data</div></strong>
                Vatsal Sharan*, Kai Sheng Tai*, Peter Bailis, Gregory Valiant</i><br>
                <i>ICML, 2019 </i><br>
                <a data-toggle="collapse" href="#collapseExample3"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1706.08146.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1706.08146"><b>arXiv</b></a> | <a href="https://github.com/kaishengtai/compressed-factorization"><b>code</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample3">
                What learning algorithms can be run directly on compressively-sensed data? If we compress a high dimensional matrix via a random projection, then what is the relationship between the factors of the original matrix and the factors of the compressed matrix? While it is well-known that random projections preserve a number of geometric properties of a dataset, this work shows that random projections can also preserve certain solutions to non-convex, NP-Hard problems like non-negative matrix factorization---both in theory and in practice.<br><br>
            </div>
 
            
            <li><p><strong><div style="font-size:15px;">Memory-Sample Tradeoffs for Linear Regression with Small Error</div></strong>
                Vatsal Sharan, Aaron Sidford, Gregory Valiant<br>
                <i>STOC, 2019</i><br>
                <a data-toggle="collapse" href="#collapseExample4"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1904.08544.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1904.08544"><b>arXiv</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample4">
                What is the role of memory in continuous optimization and learning? Are there inherent trade-offs between the available memory and the data requirement? Is it possible to achieve the sample complexity of second-order optimization methods with significantly less memory? We raise these questions, and show the first nontrivial lower bounds for linear regression with super-linear memory: we show that there is a gap between the sample complexity of algorithms with quadratic memory and that of any algorithm with sub-quadratic memory. <br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Recovery Guarantees for Quadratic Tensors with Limited Observations</div></strong>
                Hongyang Zhang, Vatsal Sharan, Moses Charikar and Yingyu Liang<br>
                <i>AISTATS, 2019</i><br>
                <a data-toggle="collapse" href="#collapseExample5"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1811.00148.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1811.00148"><b>arXiv</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample5">
               We consider the tensor completion problem of predicting the missing entries of a tensor. The commonly used CP model has a triple product form, but an alternate family of quadratic models which are the sum of pairwise products instead of a triple product have emerged from applications such as recommendation systems. Non-convex methods are the method of choice for learning quadratic models, and this work studies their sample complexity and error guarantees via theoretical results and experiments on real and synthetic data.<br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Efficient Anomaly Detection via Matrix Sketching</div></strong>
                Vatsal Sharan, Parikshit Gopalan, Udi Wieder</i><br>
                <i>NeurIPS, 2018</i><br>
                <a data-toggle="collapse" href="#collapseExample6"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1804.03065.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1804.03065"><b>arXiv</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample6">
                PCA based anomaly scores are commonly used for finding anomalies in high-dimensional data. The naive algorithms for computing these scores explicitly compute the PCA of the covariance matrix which uses space quadratic in the dimensionality of the data. Using matrix sketching tools and new matrix perturbation inequalities, we give the first streaming algorithms for computing these scores that use space that is linear or sublinear in the dimension.<br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">A Spectral View of Adversarially Robust Features</div></strong>
                Shivam Garg, Vatsal Sharan*, Brian Zhang*, Gregory Valiant</i><br>
                <i>NeurIPS, 2018 <b>(Spotlight)</i></b><br>
                <a data-toggle="collapse" href="#collapseExample7"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1811.06609.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1811.06609"><b>arXiv</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample7">
                Despite a surge of recent effort, there is still much that we do not understand about why models are vulnerable to adversarial perturbations, or how to reduce this vulnerability. In this work, we introduce the simpler, intermediate task of learning a set of "robust features"---features which are stable to adversarial perturbations. These features can subsequently be used as inputs to a classifier, which will then inherit the robustness properties. We propose one approach to constructing robust features, which leverages a tight connection between robustness and spectral properties of the neighbourhood graph of the data.<br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Moment-Based Quantile Sketches for Efficient High Cardinality Aggregation Queries</div></strong>
                Edward Gan, Jialin Ding, Kai Sheng Tai, Vatsal Sharan, Peter Bailis</i><br>
                <i>VLDB, 2018</i><br>
                <a data-toggle="collapse" href="#collapseExample8"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1803.01969.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1803.01969"><b>arXiv</b></a> | <a href="https://github.com/stanford-futuredata/msketch"><b>code</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample8">
                We propose a compact and efficiently mergeable sketch for answering quantile queries on a dataset. This data structure, which we refer to as the moments sketch, operates with a small memory footprint (200 bytes) and achieves computationally efficient (50ns) merges by tracking only a set of summary statistics, notably the sample moments.<br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Prediction with a Short Memory</div></strong>
                Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant</i><br>
                <i>STOC, 2018</i><br>
                <a data-toggle="collapse" href="#collapseExample9"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1612.02526.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1612.02526"><b>arXiv</b></a> | <a href="https://theorydish.blog/2017/11/29/prediction-with-a-short-memory/"><b>blog</b></a> | <a href="https://www.youtube.com/watch?v=-Lhmr9vqjrE&t=5s"><b>video</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample9">
                When is it necessary to remember significant information from the distant past to make good predictions about the future in sequential prediction tasks? How do we consolidate and reference memories about the past in order to predict the future? This work studies these questions, and the findings are perhaps counterintuitive: we show that a simple (Markov) model which bases its predictions on only the most recent observations and the statistics of short windows, can  achieve small error on average with respect to a large class of sequences, such as those generated by Hidden Markov Models (HMMs). This shows that long-term memory may not be necessary for making good predictions on average, even on sequences generated by complex processes that have long-range dependencies.<br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Sketching Linear Classifiers over Data Streams</div></strong>
             Kai Sheng Tai, Vatsal Sharan, Peter Bailis, Gregory Valiant</i><br>
                <i>SIGMOD, 2018</i><br>
                <a data-toggle="collapse" href="#collapseExample10"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1711.02305.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1711.02305"><b>arXiv</b></a> | <a href="https://github.com/stanford-futuredata/wmsketch"><b>code</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample10">
                We introduce a new sub-linear space sketch---the Weight-Median Sketch---for learning compressed linear classifiers over data streams while supporting the efficient recovery of large-magnitude weights in the classifier. The Weight-Median Sketch adopts the core data structure used in the Count-Sketch, but, instead of sketching counts, it captures sketched gradient updates to the model parameters. We establishes recovery guarantees for our sketch and demonstrate empirical improvements in memory-accuracy trade-offs over alternative memory-budgeted methods. <br><br>
            </div>
            
            <li><p><strong></strong><div style="font-size:15px;"><strong>Learning Overcomplete HMMs</div></strong>
            Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant</i><br>
            <i>NeurIPS, 2017</i><br>
            <a data-toggle="collapse" href="#collapseExample11"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1711.02309.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1711.02309"><b>arXiv</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample11">
                We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. <br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use
            </div></strong>
            Vatsal Sharan, Gregory Valiant</i><br>
            <i>ICML, 2017</i><br>
            <a data-toggle="collapse" href="#collapseExample12"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1703.01804.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1703.01804"><b>arXiv</b></a> | <a href="http://web.stanford.edu/~vsharan/orth-als.html"><b>code</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample12">
                The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is efficient and easy to implement, but often converges to poor local optima. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS on a variety of synthetic and real-world tasks. <br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Large Deviation Property for Waiting Times for Markov and Mixing Processes
            </div></strong>
            Vatsal Sharan, R.K. Bansal</i><br>
            <i>ISIT, 2014</i><br>
            <a data-toggle="collapse" href="#collapseExample13"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1703.01804.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1703.01804"><b>arXiv</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample13">
                We show a concentration theorem for the waiting time until the opening string in the realization of a random process first appears in an independent realization of the same or a different process.<br><br>
            </div>
            
            </ol>
            <br>
            During undergrad, I did some work on localization in sensor networks.<br><br>
            <ol>
            <li><p><strong><div style="font-size:15px;">Localization of Acoustic Beacons using Iterative Null Beamforming over Ad-hoc Wireless Sensor Networks
            </div></strong>
            Vatsal Sharan, Sudhir Kumar, and Rajesh Hegde<br>
            <i>IEEE Asilomar Conference on Signals, Systems, and Computers, 2013</i><br>
            <a data-toggle="collapse" href="#collapseExample14"><b>abstract</b></a> | <a href="http://web.stanford.edu/~vsharan/asilomar.pdf"><b>pdf</b></a>
                </p></li>
            
            <div class="collapse" id="collapseExample14">
                We propose an iterative method to localize and separate multiple audio beacons using the principle of null beam forming.<br><br>
            </div>
            
            <li><p><strong><div style="font-size:15px;">Energy Efficient Optimal Node-Source Localization using Mobile Beacon in Ad-Hoc Sensor Networks
            </div></strong>
            Sudhir Kumar, Vatsal Sharan and Rajesh Hegde<br>
            <i>IEEE Global Communications Conference (Globecom), 2013</i><br>
            <a data-toggle="collapse" href="#collapseExample14"><b>abstract</b></a> | <a href="http://web.stanford.edu/~vsharan/globecom.pdf"><b>pdf</b></a>                 </p></li>
            
            <div class="collapse" id="collapseExample15">
                We propose a single mobile beacon based method to localize nodes using the principle of maximum power reception.<br><br>
            </div>

            </ol>



<br><br><br>



	  </section>
	 
	  
	</div>
      </div>
    </div><!-- /.container -->

 

  
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.3.min.js"></script>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    
  </body>
</html>
