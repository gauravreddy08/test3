<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    
    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="css/starter-template.css" rel="stylesheet">
        
    <link href="css/main.css" rel="stylesheet">
    
        <title>CS699: Theory of ML</title>
  </head>

<body> o
<div class="container">

	    	<div class="row">
            <div class="col-lg-12">
                <h1 class="page-header" style="color:#B03A2E;">CSCI 699: Theory of Machine Learning
                </h1>
            </div>
        </div>

<p>

<h3 style="color:#1F618D;">Basic Information</h3>

<ul>
    <li> <b> Lecture time: </b> Monday, Wednesday 10:00 am to 11:50 am
    <li> <b> Lecture place: </b> KAP 141
    <li> <b> Instructor: </b> Vatsal Sharan
        <ul>
            <li> <b> Office: </b> SAL 220
                <li> <b> Office Hours: </b> By appointment.
                <li> <b> Contact: </b> Fastest response will be via Piazza, but you can also email me at vsharan at usc.edu.
        </ul>
        <li> <b> Piazza: </b> We will be using Piazza for all course communications (regarding homework, project, course scheduling, etc). Please feel free to ask/answer any questions about the class on Piazza. You can post privately on Piazza to contact the course staff for any reason. Signup for Piazza  <a href="https://piazza.com/class/ksi4siosqfs5x2">here</a>.
            <li> <b> Gradescope: </b> We will use <a href="https://gradescope.com/">Gradescope </a> for assignment and final project submission. Please create an account on Gradescope using your USC ID and join CSCI 699 using entry code KYXX4E.
</ul>

<h3 style="color:#1F618D;">Course Description and Objectives</h3>

<p> This course focuses on the theoretical foundation of machine learning. The focus will be on understanding fundamental questions regarding both computational and statistical aspects of learning, with an overarching goal to answer the core question: what is the complexity of learning, in terms of its computational and statistical requirements? The course will also cover several modern aspects of the theory of maching learning---including memory complexity of learning, deep learning theory, robustness, and fairness.</p>

<p> The hope is that through this course you will learn to think about machine learning in a more rigorous and principled way and have the skills to design provable and practical machine learning algorithms, which also work in the face of various computational and statistical constraints and meet modern desiderata. The course will equip you with the tools required to undertake research in theoretical machine learning, and will shed light on various interesting research frontiers.</p>

<h3 style="color:#1F618D;">Prerequisites</h3>

Familiarity with probability, linear algebra, calculus, analysis of algorithms, basic understanding of machine learning.

           
<h3 style="color:#1F618D;">Syllabus and Materials</h3>

We will post lecture notes and assignments here. Additional related reading for all lectures will be posted on Piazza after the lecture.
<br><br>
<div id="table-custom">
<table style="width:100%" align="right">
<tr>
  <th>Lecture</th>
  <th>Topics</th>
  <th>Lecture notes</th>
  <th>Homework</th>
</tr>
<tr>
  <td align="center">1, 08/23</td>
  <td align="left">Introduction, PAC learning </td>
  <td align="left">
  <a href="/lecture_notes/lec1.pdf">Class notes</a>, <a href="/lecture_notes/lec1_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">2, 08/25</td>
  <td align="left">Finite hypothesis classes, Bias-Complexity tradeoff, Agnostic PAC learning
  <td align="left">
      <a href="/lecture_notes/lec2.pdf">Class notes</a>, <a href="/lecture_notes/lec2_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">3, 08/30</td>
  <td align="left">Uniform convergence, concentration inequalities
  <td align="left">
      <a href="/lecture_notes/lec3.pdf">Class notes</a>, <a href="/lecture_notes/lec3_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">4, 09/01</td>
  <td align="left">Concentration inequalities, sub-Gaussian random variables
  <td align="left">
      <a href="/lecture_notes/lec4.pdf">Class notes</a>, <a href="/lecture_notes/lec4_final.pdf">Scribe notes</a>
  </td>
  <td align="center"><a href="/lecture_notes/hw1.pdf">HW1</a></td>
</tr>
<tr>
  <td align="center">5, 09/08</td>
  <td align="left">VC dimension
  <td align="left">
      <a href="/lecture_notes/lec5.pdf">Class notes</a>, <a href="/lecture_notes/lec5_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">6, 09/13</td>
  <td align="left">VC theorem, Rademacher complexity
  <td align="left">
      <a href="/lecture_notes/lec6.pdf">Class notes</a>, <a href="/lecture_notes/lec6_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">7, 09/15</td>
  <td align="left">Generalization bounds using Rademacher complexity, properties of Rademacher complexity, linear hypothesis classes
  <td align="left">
      <a href="/lecture_notes/lec7.pdf">Class notes</a>, <a href="/lecture_notes/lec7_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">8, 09/20</td>
  <td align="left">Start computational complexity of learning, learning conjunctions efficiently
  <td align="left">
      <a href="/lecture_notes/lec8.pdf">Class notes</a>, <a href="/lecture_notes/lec8_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">9, 09/22</td>
  <td align="left">Intractability of learning 3-term DNF, Using 3-CNF to avoid intractability, proper vs. improper learning
  <td align="left">
      <a href="/lecture_notes/lec9.pdf">Class notes</a>, <a href="/lecture_notes/lec9_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">10, 09/27</td>
  <td align="left">Hardness of improper learning using crypotographic assumptions, hardness of agnostically learning halfspaces
  <td align="left">
      <a href="/lecture_notes/lec10.pdf">Class notes</a>, <a href="/lecture_notes/lec10_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">11, 09/29</td>
  <td align="left">Learning with random classification noise (RCN), SQ learning
  <td align="left">
      <a href="/lecture_notes/lec11.pdf">Class notes</a>, <a href="/lecture_notes/lec11_final.pdf">Scribe notes</a>
  </td>
  <td align="center"><a href="/lecture_notes/hw2.pdf">HW2</a></td>
</tr>
<tr>
  <td align="center">12, 10/04</td>
  <td align="left">SQ learning implies learning with RCN, SQ dimension, learning parities
  <td align="left">
      <a href="/lecture_notes/lec12.pdf">Class notes</a>, <a href="/lecture_notes/lec12_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">13, 10/06</td>
  <td align="left">Hardness of learning parities in SQ, begin boosting
  <td align="left">
      <a href="/lecture_notes/lec13.pdf">Class notes</a>, <a href="/lecture_notes/lec13_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">14, 10/11</td>
  <td align="left">AdaBoost and boosting, convex optimization, convex learning problems and convex surrogates
  <td align="left">
      <a href="/lecture_notes/lec14.pdf">Class notes</a>, <a href="/lecture_notes/lec14_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">15, 10/13</td>
  <td align="left">Consistency of convex surrogates, gradient descent (GD), convergence of GD, variants and properties of GD
  <td align="left">
      <a href="/lecture_notes/lec15.pdf">Class notes</a>, <a href="/lecture_notes/lec15_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">16, 10/18</td>
  <td align="left">SGD and its convergence, learning with SGD, begin online learning, mistake bound model
  <td align="left">
      <a href="/lecture_notes/lec16.pdf">Class notes</a>, <a href="/lecture_notes/lec16_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">17, 10/20</td>
  <td align="left">Littlestone dimension, online learning in the unrealizable case, Weighted Majoriry algorithm
  <td align="left">
      <a href="/lecture_notes/lec17.pdf">Class notes</a>, <a href="/lecture_notes/lec17_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">18, 10/25</td>
  <td align="left">Online convex optimization, Follow-the-Leader, Follow-the-Regularized-Leader, Online gradient descent (OGD)
  <td align="left">
      <a href="/lecture_notes/lec18.pdf">Class notes</a>, <a href="/lecture_notes/lec18_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">19, 10/27</td>
  <td align="left">Regret of OGD, online perceptron, computational-statistical tradeoffs, planted clique problem
  <td align="left">
      <a href="/lecture_notes/lec19.pdf">Class notes, <a href="/lecture_notes/lec19_final.pdf">Scribe notes</a></a>
  </td>
  <td align="center"><a href="/lecture_notes/hw3.pdf">HW3</a></td>
</tr>
<tr>
  <td align="center">20, 11/01</td>
  <td align="left">Planted clique conjecture, memory-sample tradeoffs for learning. Class presentation begin. See schedule <a href="/project.html">here</a>.
  <td align="left">
      <a href="/lecture_notes/lec20.pdf">Class notes</a>, <a href="/lecture_notes/lec20_final.pdf">Scribe notes</a>
  </td>
  <td align="center"></td>
</tr>
<tr>
  <td align="center">21, 11/03</td>
  <td align="left">See presentation schedule for remainder of semester <a href="/project.html">here</a>.
  <td align="center"></td>
  <td align="center"></td>
</tr>
</table>
</div>
     
<div><h3 style="color:#1F618D;">Requirements and Grading</h3></div>

<ol>
    
    <li>  <b> 3-4 homeworks </b> worth 40% of the grade. Discussion is allowed and encouraged but everyone should write solutions on their own. Homeworks should be written in Latex and submitted via Gradescope.
        
        <li> The major component will be a <b> course presentation </b> (30%) and <b> project </b> (25%). An overview of the requirements is given below, detailed instructions will be discussed later.
            
            <ul> <li>The student presentations are one of the most important components of the class, since they will bring you to the forefront of ML theory research. You will be matched to a paper to present in class based on your interest and will be expected to make a 30 minute presentation on the topic. To help ensure all presentations are high quality, presenters will be asked to go over their slides and review their preparation with the course staff a week before the presentation (this will be worth 10%, the actual in-class presentation will be worth 20%).
                 <li> For the project, you will be expected to write a 8-9 page report on 1-2 papers. This could be 1) on the paper you presented, plus related reading; or 2) you are free to choose a different paper(s). You can also do research on ML theory as part of your project, please discuss this with me if this is of interest.
                     </ul>
            <li> 5% of the grade will be based on <b> course attendance and participation </b>.
</ol>

<h3 style="color:#1F618D;">Resources and related courses</h3>

<ol>
    
    <li>  There is no required textbook for this class, but the following books are good supplemental reading for many parts.
        <ul>
            
            <li> Understanding Machine Learning:
                From Theory to Algorithms, by Shai Shalev-Shwartz and Shai Ben-David. Availble online <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">here</a>.
                
                <li> An Introduction to Computational Learning Theory, by Michael J. Kearns and Umesh Vazirani. Available using your USC account
                    <a href="https://uosc.primo.exlibrisgroup.com/discovery/fulldisplay?docid=cdi_proquest_ebookcentral_EBC5966119&context=PC&vid=01USC_INST:01USC&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&mode=Basic">here</a>.
                    
           </ul>
        
        <li> This course draws heavily from several other related courses, and the material there could be a good reference. Here are some pointers (there are also many other excellent courses missing in this short list):
            
            <ul>
             
             <li> Haipeng Luo's class at USC. <a href="https://haipeng-luo.net/courses/CSCI699_2019/index.html">[website]</a>
                 
                 <li> Akshay Krishnamurthy's class at UMass.
                     <a href="https://people.cs.umass.edu/~akshay/courses/cs690m/index.html">[website]</a>
                     <li> Tengyu Ma's and Percy Liang's class at Stanford.
                     <a href="http://web.stanford.edu/class/stats214/">[Tengyu's course website]</a> <a href="https://web.stanford.edu/class/cs229t/2017/Lectures/percy-notes.pdf">[Percy's lecture notes]</a>
                     
                     <li> Rocco Servedio's class at Columbia.
                     <a href="http://www.cs.columbia.edu/~cs4252/">[website]</a>
                     
                     <li> Varun Kanade's class at Oxford.
                     <a href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/CLT-HT-TT2021/">[website]</a>
                     
                     <li> Avrim Blum's class at TTIC. <a href="https://home.ttic.edu/~avrim/MLT20/">[website]</a>
                     
                     <li> Matus Telgarsky's class at UIUC.
                     <a href="https://mjt.cs.illinois.edu/courses/mlt-f18/">[website]</a>
                     
                     
                     
                     
            </ul>
            
         </ol>
     
      <br><br><br>
     
</div>
</div>
</body>
</html>
