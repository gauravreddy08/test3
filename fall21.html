<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="css/starter-template.css" rel="stylesheet">
        <title>CS699: Theory of ML</title>
  </head>

<body>
<div class="container">

	    	<div class="row">
            <div class="col-lg-12">
                <h1 class="page-header" style="color:#B03A2E;">CSCI 699: Theory of Machine Learning
                </h1>
            </div>
        </div>

<p>

<h3 style="color:#1F618D;">Basic Information</h3>

<ul>
    <li> <b> Lecture time: </b> Monday, Wednesday 10:00 am to 11:50 am
    <li> <b> Lecture place: </b> KAP 141
    <li> <b> Instructor: </b> Vatsal Sharan
        <ul>
            <li> <b> Office: </b> SAL 220
                <li> <b> Office Hours: </b> By appointment.
                <li> <b> Contact: </b> Fastest response will be via Piazza, but you can also email me at vsharan at usc.edu.
        </ul>
        <li> <b> Piazza: </b> We will be using Piazza for all course communications (regarding homework, project, course scheduling, etc). Please feel free to ask/answer any questions about the class on Piazza. You can post privately on Piazza to contact the course staff for any reason. Signup for Piazza  <a href="https://piazza.com/class/ksi4siosqfs5x2">here</a>.
            <li> <b> Gradescope: </b> We will use <a href="https://gradescope.com/">Gradescope </a> for assignment and final project submission. Please create an account on Gradescope using your USC ID and join CSCI 699 using entry code KYXX4E.
</ul>

<h3 style="color:#1F618D;">Course Description and Objectives</h3>

<p> This course focuses on the theoretical foundation of machine learning. The focus will be on understanding fundamental questions regarding both computational and statistical aspects of learning, with an overarching goal to answer the core question: what is the complexity of learning, in terms of its computational and statisitical requirements? The course will also cover several modern aspects of the theory of maching learning---including memory complexity of learning, deep learning theory, robustness, and fairness.</p>

<p> The hope is that through this course you will learn to think about machine learning in a more rigorous and principled way and have the skills to design provable and practical machine learning algorithms, which also work in the face of various computational and statistical constraints and meet modern desiderata. The course will equip you with the tools required to undertake research in theoretical machine learning, and will shed light on various interesting research frontiers.</p>

<h3 style="color:#1F618D;">Prerequisites</h3>

Familiarity with probability, linear algebra, calculus, analysis of algorithms, basic understanding of machine learning.

<h3 style="color:#1F618D;">Topics</h3>

<p>We will begin by covering basics of statistical learning theory, in particular the statistical requirement for solving learning tasks. We will cover the following fundamental tools:</p>
<ol>
    <li> Introduction to supervised learning, PAC learning
    <li> Uniform convergence, finite hypothesis classes
    <li> Uniform covergence for parametrized hypothesis classes, covering numbers
    <li> Concentration inequalities, statistics background
    <li> Agnostic learning, bias complexity tradeoff
    <li> VC dimension, Sauer's lemma
    <li> Rademacher complexity, chaining technique, Dudley entropy integral
</ol>
    
    <p>Next, we will shift our focus to the computational side of things. We will cover the following topics (many of these fall under the umbrella of computational learning theory):</p>
    
    <ol>
        <li> Basics of computational complexity, runtime of empirical risk minimization
            <li> Learning conjunctions, intractability of learning 3-term DNF, proper vs. impoper learning
            <li> Computational hardness of learning
             <li> Learning in the presence of noise, Statistical Query (SQ) model, lower bounds on SQ learning
             <li> Basics of online learning
             <li> Optimization for machine learning, analysis of gradient descent for convex optimization
    </ol>
    
    <p>The third part of the course (short, around 2 weeks) will cover tradeoffs between computational and statistical requirements:</p>
    
    <ol>
        <li> Computational-statistical tradeoffs for statistical inference, the planted clique problem
            <li> Memory-sample tradoffs for learning
    </ol>
        
        <p>Finally, we will move onto student presentations in the last (roughly) three weeks. We will cover a subset of the following topics (depending on student interest):</p>
        
        <ol>
            <li> Theory of deep learning, generalization for deep neural networks
                <li> Covergence of gradient descent for non-convex problems
                    <li> Robust machine learning, robustness to corruptions in training data, robustness to test-time adversarial perturbations
                        <li> Domain adaptation and out-of-domain generalization
                        <li> Bias in machine learning, algorithmic fairness
        </ol>

           <p> The final list of topics could change depending on our pace and the interests of the class. There is no official textbook for this course. Lecture notes and related reading will be posted on this website after each class.</p>
           
<h3 style="color:#1F618D;">Lecture Notes</h3>

<ol>
    <li> Whiteboard notes from lecture 1 are available <a href="/lecture_notes/lec1.pdf">here</a>.
        
    <li> Whiteboard notes from lecture 1 are available <a href="/lecture_notes/lec2.pdf">here</a>.
</ol>
            
<h3 style="color:#1F618D;">Requirements and Grading</h3>

<ol>
    
    <li>  <b> 3-4 homeworks </b> worth 40% of the grade. Discussion is allowed and encouraged but everyone should write solutions on their own. Homeworks should be written in Latex and submitted via Gradescope.
        
        <li> The major component will be a <b> course presentation </b> (30%) and <b> project </b> (25%). An overview of the requirements is given below, detailed instructions will be discussed later.
            
            <ul> <li>The student presentations are one of the most important components of the class, since they will bring you to the forefront of ML theory research. You will be matched to a paper to present in class based on your interest and will be expected to make a 30 minute presentation on the topic. To help ensure all presentations are high quality, presenters will be asked to go over their slides and review their preparation with the course staff a week before the presentation (this will be worth 10%, the actual in-class presentation will be worth 20%).
                 <li> For the project, you will be expected to write a 8-9 page report on 1-2 papers. This could be 1) on the paper you presented, plus related reading; or 2) you are free to choose a different paper(s). You can also do research on ML theory as part of your project, please discuss this with me if this is of interest.
                     </ul>
            <li> 5% of the grade will be based on <b> course attendance and participation </b>.
         </ol>
     
      <br><br><br>
     
</div>
</div>
</body>
</html>
